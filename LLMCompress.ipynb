{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b583b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fracture/anaconda3/envs/LMCompress+/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Iterator\n",
    "from arithmetic_coder import arithmetic_coder, ac_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43be0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class Metric:\n",
    "    def __init__(self):\n",
    "        self.total_length = 0\n",
    "        self.compressed_length = 0\n",
    "\n",
    "    def compute_ratio(self):\n",
    "        if self.total_length != 0 and self.compressed_length != 0:\n",
    "            return (\n",
    "                self.total_length / self.compressed_length,\n",
    "                self.compressed_length / self.total_length,\n",
    "            )\n",
    "        else:\n",
    "            return 0, 0\n",
    "\n",
    "    def accumulate(self, compressed, original):\n",
    "        if isinstance(compressed, list):\n",
    "            self.compressed_length += len(compressed)\n",
    "        elif isinstance(compressed, int):\n",
    "            self.compressed_length += compressed\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported compressed length type: {type(compressed)}\")\n",
    "\n",
    "        if isinstance(original, list):\n",
    "            self.total_length += len(original)\n",
    "        elif isinstance(original, int):\n",
    "            self.total_length += original\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported original length type: {type(original)}\")\n",
    "\n",
    "\n",
    "def compress(compress_input, logits, metric):\n",
    "    \"\"\"\n",
    "    :param compress_input: symbols to be compressed\n",
    "    :param logits: generation probabilities from the model\n",
    "    :param metric: compression metrics\n",
    "    :return: compressed result, a floating number\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    # Initialize a Encoder Object\n",
    "    # Precision is for the encoder, not the model\n",
    "    # You must have the same precision for encoder and decoder\n",
    "    # Tricky things here: Though theoratically prefill == decode, but in practice there are numerical problems\n",
    "    encoder = arithmetic_coder.Encoder(\n",
    "        base=2,\n",
    "        precision=64,\n",
    "        output_fn=output.append,\n",
    "    )\n",
    "    # the first symbol should be saved for generation in decoding\n",
    "    start_symbol = compress_input[:, :1]\n",
    "    probs = logits.softmax(dim=-1).to(torch.float32)\n",
    "    pd = torch.gather(probs, dim=-1, index=compress_input[:, 1:].unsqueeze(-1)).squeeze(\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    probs = np.vstack(probs.detach().cpu().numpy().squeeze())\n",
    "\n",
    "    sequence_array = compress_input[:, 1:].detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "    pd = pd.squeeze()\n",
    "\n",
    "    # compress the sequence\n",
    "    for symbol, prob, pd_prob in zip(sequence_array, probs, pd):\n",
    "        encoder.encode(\n",
    "            ac_utils.normalize_pdf_for_arithmetic_coding(prob, np.float32), symbol\n",
    "        )\n",
    "    encoder.terminate()\n",
    "\n",
    "    # to visualize and compute metrics, map to str\n",
    "    compressed_bits = \"\".join(map(str, output))\n",
    "    # you can only save in bytes, so need to pad some bits\n",
    "    compressed_bytes, num_padded_bits = ac_utils.bits_to_bytes(compressed_bits)\n",
    "    metric.accumulate(len(compressed_bytes) + num_padded_bits, len(sequence_array))\n",
    "\n",
    "    compress_rate, compress_ratio = metric.compute_ratio()\n",
    "    logger.info(f\"compressed length: {metric.compressed_length}\")\n",
    "    logger.info(f\"original length: {metric.total_length}\")\n",
    "    logger.info(f\"compression ratio: {compress_ratio:.6f}\")\n",
    "    logger.info(f\"compression rate: {compress_rate:.6f}\")\n",
    "\n",
    "    return compressed_bytes, num_padded_bits, start_symbol, sequence_array, pd, probs\n",
    "\n",
    "\n",
    "def decode(\n",
    "    compressed_bytes,\n",
    "    num_padded_bits,\n",
    "    model,\n",
    "    start_symbol,\n",
    "    device,\n",
    "    original_seq_len,\n",
    "    original_sequence=None,\n",
    "    pd=None,\n",
    "    probs=None,\n",
    "    do_test=True,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param compressed_bytes:  compressed data\n",
    "    :param num_padded_bits:  padded bits\n",
    "    :param model: same model as encoder\n",
    "    :param start_symbol: first symbol to generate\n",
    "    :param original_sequence: original symbol sequence, for testing purpose\n",
    "    :param pd: actually not needed, used for testing\n",
    "    :param probs:\n",
    "    :param device:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # convert bytes back to bit stream\n",
    "    data_iter = iter(\n",
    "        ac_utils.bytes_to_bits(compressed_bytes, num_padded_bits=num_padded_bits)\n",
    "    )\n",
    "\n",
    "    # utils function to read bits\n",
    "    def _input_fn(bit_sequence: Iterator[str] = data_iter) -> int | None:\n",
    "        try:\n",
    "            return int(next(bit_sequence))\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "    # initialize a Decoder Object\n",
    "    decoder = arithmetic_coder.Decoder(\n",
    "        base=2,\n",
    "        precision=64,\n",
    "        input_fn=_input_fn,\n",
    "    )\n",
    "\n",
    "    sequence_array_de = start_symbol.squeeze(0).detach().cpu().numpy()\n",
    "    sequence_array_de_input = start_symbol\n",
    "    target_diff_list = []\n",
    "    target_in_top5_list = []\n",
    "\n",
    "    # loop for decompressing\n",
    "    # pad the input to the original length\n",
    "    sequence_array_de_input = torch.tensor(sequence_array_de_input, dtype=torch.long, device=device)\n",
    "    sequence_array_de_input = torch.nn.functional.pad(sequence_array_de_input, (0, original_seq_len-1), value=0)\n",
    "\n",
    "    for i in range(original_seq_len):\n",
    "        # attention_mask = (sequence_array_de_input != 0).long()\n",
    "        with torch.no_grad():\n",
    "            logits = model(sequence_array_de_input, use_cache=False).logits.to(\n",
    "                torch.float32\n",
    "            )\n",
    "        # get generaton probabilities, decode the next token\n",
    "        prob_de = logits.softmax(dim=-1).detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "        de_token = decoder.decode(\n",
    "            ac_utils.normalize_pdf_for_arithmetic_coding(prob_de[i], np.float32)\n",
    "        )\n",
    "        # using the original probs to decode, for testing purpose\n",
    "        # de_token = decoder.decode(ac_utils.normalize_pdf_for_arithmetic_coding(probs[i]))\n",
    "        # append to the generated sequence\n",
    "        sequence_array_de = np.append(sequence_array_de, de_token)\n",
    "\n",
    "        current_len = len(sequence_array_de)\n",
    "        target_len = original_seq_len\n",
    "\n",
    "        if current_len < target_len:\n",
    "            padded = np.pad(\n",
    "                sequence_array_de, (0, (target_len - current_len)), constant_values=0\n",
    "            )\n",
    "        else:\n",
    "            padded = sequence_array_de\n",
    "        sequence_array_de_input = torch.tensor(\n",
    "            padded, dtype=torch.long, device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        if do_test:\n",
    "            top_indices_de = prob_de[i].argsort()[-5:][::-1]\n",
    "            top_indices = probs[i].argsort()[-5:][::-1]\n",
    "\n",
    "            # target diff\n",
    "            target_diff = probs[i, original_sequence[i]] - prob_de[i, original_sequence[i]]\n",
    "            target_diff_list.append(target_diff)\n",
    "\n",
    "            # target in top 5\n",
    "            target_in_top5 = original_sequence[i] in top_indices\n",
    "            target_in_top5_list.append(target_in_top5)\n",
    "            print(\n",
    "                f\"idx: {i}, original token: {original_sequence[i]}, decoder token: {de_token}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"diff probs max: {max(abs(probs[i] - prob_de[i]))}, original sum error: {abs(sum(prob_de[i]) - 1.0)}, decoder sum error: {abs(sum(probs[i]) - 1.0)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"original: {top_indices}, target_in_top5: {target_in_top5} decode: {top_indices_de}, \"\n",
    "            )\n",
    "            print(f\"target diff: {target_diff}\")\n",
    "            if original_sequence[i] != de_token:\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "        \n",
    "    return sequence_array_de_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178b66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_padded_bytes(filename: str, data: bytes, num_padded_bits: int, original_length: int):\n",
    "    \"\"\"\n",
    "    file format:\n",
    "    - first byte: number of padded bit\n",
    "    - second and third byte: original length (usually, llm context will not exceed 65535)\n",
    "    - subsequent bytes: actual bytes data\n",
    "\n",
    "    :param filename: output file name\n",
    "    :param data: bytes data to write\n",
    "    :param padding_bits: number of padded bits (must be between 0 and 7)\n",
    "    :param original_length: original length of the uncompressed data (in tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    if not 0 <= num_padded_bits <= 7:\n",
    "        raise ValueError(\"num_padded_bits must be between 0 and 7.\")\n",
    "\n",
    "    if not 0 <= original_length <= 65535:\n",
    "        raise ValueError(\"original_length must be between 0 and 65535.\")\n",
    "\n",
    "    if not isinstance(data, bytes):\n",
    "        raise TypeError(\"data must be of bytes type.\")\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        padding_byte = num_padded_bits.to_bytes(1, 'big')\n",
    "        f.write(padding_byte)\n",
    "        f.write(original_length.to_bytes(2, 'big'))\n",
    "        f.write(data)\n",
    "\n",
    "def read_padded_bytes(filename: str) -> tuple[bytes, int]:\n",
    "    \"\"\"\n",
    "    Read data and padding bits from a file.\n",
    "\n",
    "    :param filename: The name of the file to read.\n",
    "    :return: A tuple containing (bytes data, number of padded bits).\n",
    "             May raise an error if the file is empty or improperly formatted.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        # the first byte indicates the number of padded bits\n",
    "        padding_byte = f.read(1)\n",
    "\n",
    "        # If the file is empty, f.read(1) will return an empty bytes object b''\n",
    "        if not padding_byte:\n",
    "            raise EOFError(\"File is empty or improperly formatted: unable to read padding bits byte.\")\n",
    "\n",
    "        original_length_bytes = f.read(2)\n",
    "        if not original_length_bytes:\n",
    "            raise EOFError(\"File is empty or improperly formatted: unable to read original length bytes.\")\n",
    "    \n",
    "        padding_bits = int.from_bytes(padding_byte, 'big')\n",
    "        original_length = int.from_bytes(original_length_bytes, 'big')\n",
    "\n",
    "        data = f.read()\n",
    "        \n",
    "        return data, padding_bits, original_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x06\\x03\\xce\\xde\\xa9v\\n\\x84\\xdc\\xaf\\x02\\x04\\xf7\\xfbuh.\\x85\\xad\\xb4@\\xf6'\n",
      "4\n",
      "45\n",
      "b'\\x06\\x03\\xce\\xde\\xa9v\\n\\x84\\xdc\\xaf\\x02\\x04\\xf7\\xfbuh.\\x85\\xad\\xb4@\\xf6'\n",
      "4\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1739/665979826.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_array_de_input = torch.tensor(sequence_array_de_input, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, original token: 264, decoder token: 264\n",
      "diff probs max: 0.0, original sum error: 0.00035369396209716797, decoder sum error: 0.00035369396209716797\n",
      "original: [  264   279   419   458 21495], target_in_top5: True decode: [  264   279   419   458 21495], \n",
      "target diff: 0.0\n",
      "idx: 1, original token: 33634, decoder token: 33634\n",
      "diff probs max: 0.0, original sum error: 0.00024968385696411133, decoder sum error: 0.00024968385696411133\n",
      "original: [ 6888  3654  2613 24017  8038], target_in_top5: False decode: [ 6888  3654  2613 24017  8038], \n",
      "target diff: 0.0\n",
      "idx: 2, original token: 9271, decoder token: 9271\n",
      "diff probs max: 0.0, original sum error: 0.0003032088279724121, decoder sum error: 0.0003032088279724121\n",
      "original: [ 2484   501  4401  3271 38910], target_in_top5: False decode: [ 2484   501  4401  3271 38910], \n",
      "target diff: 0.0\n",
      "idx: 3, original token: 11, decoder token: 11\n",
      "diff probs max: 0.0, original sum error: 0.00023871660232543945, decoder sum error: 0.00023871660232543945\n",
      "original: [ 11 504 429 369 304], target_in_top5: True decode: [ 11 504 429 369 304], \n",
      "target diff: 0.0\n",
      "idx: 4, original token: 13923, decoder token: 13923\n",
      "diff probs max: 0.0, original sum error: 0.00032013654708862305, decoder sum error: 0.00032013654708862305\n",
      "original: [  264   279 11811 13923   432], target_in_top5: True decode: [  264   279 11811 13923   432], \n",
      "target diff: 0.0\n",
      "idx: 5, original token: 11105, decoder token: 11105\n",
      "diff probs max: 0.0, original sum error: 0.00013560056686401367, decoder sum error: 0.00013560056686401367\n",
      "original: [ 614  518  504 1977  304], target_in_top5: False decode: [ 614  518  504 1977  304], \n",
      "target diff: 0.0\n",
      "idx: 6, original token: 264, decoder token: 264\n",
      "diff probs max: 6.109476089477539e-07, original sum error: 0.0002270340919494629, decoder sum error: 0.00022709369659423828\n",
      "original: [ 429  264  279  458 5904], target_in_top5: True decode: [ 429  264  279  458 5904], \n",
      "target diff: 6.109476089477539e-07\n",
      "idx: 7, original token: 58113, decoder token: 58113\n",
      "diff probs max: 1.341104507446289e-07, original sum error: 0.00027805566787719727, decoder sum error: 0.0002778172492980957\n",
      "original: [  501  8597   220 16770 14862], target_in_top5: False decode: [  501  8597   220 16770 14862], \n",
      "target diff: -2.6193447411060333e-10\n",
      "idx: 8, original token: 315, decoder token: 315\n",
      "diff probs max: 1.1920928955078125e-07, original sum error: 0.00015234947204589844, decoder sum error: 0.00015228986740112305\n",
      "original: [ 315 3563  429  220 2832], target_in_top5: True decode: [ 315 3563  429  220 2832], \n",
      "target diff: -1.1920928955078125e-07\n",
      "idx: 9, original token: 87549, decoder token: 95386\n",
      "diff probs max: 2.5331974029541016e-07, original sum error: 0.00028067827224731445, decoder sum error: 0.0002804994583129883\n",
      "original: [  220 60766 14538 50933  8380], target_in_top5: False decode: [  220 60766 14538 50933  8380], \n",
      "target diff: -3.4924596548080444e-09\n",
      "> \u001b[32m/tmp/ipykernel_1739/665979826.py\u001b[39m(\u001b[92m140\u001b[39m)\u001b[36mdecode\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    138\u001b[39m     sequence_array_de_input = torch.nn.functional.pad(sequence_array_de_input, (\u001b[32m0\u001b[39m, original_seq_len-\u001b[32m1\u001b[39m), value=\u001b[32m0\u001b[39m)\n",
      "\u001b[32m    139\u001b[39m \n",
      "\u001b[32m--> 140\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m range(original_seq_len):\n",
      "\u001b[32m    141\u001b[39m         \u001b[38;5;66;03m# attention_mask = (sequence_array_de_input != 0).long()\u001b[39;00m\n",
      "\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# model and tokenizer loading\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"pretrained/Qwen2.5-0.5B\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pretrained/Qwen2.5-0.5B\", use_fast=False)\n",
    "llm.eval()\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# data\n",
    "# sample_text = \"Super simple text to be tested.\"\n",
    "# sample_text = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "sample_text = r\"\"\"Greenhouse gas emissions from the burning of fossil fuels have pushed the acidity of the world's oceans past a safe threshold, scientists warn, threatening their ability to sustain shellfish and corals and help us in the fight against climate change. A new report says that ocean acidification is the latest \"planetary boundary\" to be crossed, a reference to a set of warning signs related to key planetary systems that keep the Earth safe for human civilization. Other planetary boundaries that have already been crossed — including dangerous levels of chemical pollution, the warming atmosphere and changes to the nutrient cycle — have already signalled threats to people.  \"Go outside of these boundaries and you first enter a danger zone, with higher risk of causing changes that would undermine that ability to support human life and human development,\" said Johan Rockström, director of the Potsdam Institute for Climate Impact Research, which is behind the Planetary Health Check report released on Wednesday. \"And once you are at the upper end of the uncertainty range ... you enter the red zone, the high-risk zone where most science agrees that we are very likely to depress buttons that will cause irreversible changes, basically committing ourselves to drifting away from livable conditions on Earth.\" Adding the oceans to the planetary boundaries list is a major concern because of the billions of people who depend on them. Continuing ocean acidification could not only destroy fisheries that people rely on for food but reduce the ability of the ocean to absorb carbon dioxide and moderate global warming. As humans burn fossil fuels and pump carbon dioxide into the atmosphere, it's estimated that the ocean is absorbing more than a quarter of that CO2.  \"Just like when we add carbon dioxide to Coke or soda, that makes the soft drink more acidic,\" said Christopher Harley, a professor who studies climate change and the ocean at the University of British Columbia.  But when CO2 is absorbed, the chemical process effectively lowers the availability of a mineral that certain marine life — from shellfish to coral — need to develop their bodies. \"It makes it harder to build shells — and you need to add shell if you want to grow bigger,\" Harley explained, comparing it to the construction of a house.  \"All of a sudden, the building materials become more costly. You're either going to build smaller homes or not as many.\" \"\"\"\n",
    "\n",
    "# work flow\n",
    "compression_start_time = time.time()\n",
    "\n",
    "tokenized = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "metric = Metric()\n",
    "with torch.inference_mode():\n",
    "    # we don't need the last token's logits\n",
    "    logits = (\n",
    "        llm(tokenized[\"input_ids\"], use_cache=False).logits[:, :-1].to(torch.float32)\n",
    "    )\n",
    "compressed_bytes, num_padded_bits, start_symbol, sequence_array, pd, probs = compress(\n",
    "    tokenized[\"input_ids\"], logits, metric\n",
    ")\n",
    "\n",
    "compression_end_time = time.time()\n",
    "\n",
    "print(compressed_bytes)\n",
    "print(num_padded_bits)\n",
    "original_length = tokenized[\"input_ids\"].shape[1] - 1\n",
    "print(original_length)\n",
    "write_padded_bytes(\"compressed.bin\", compressed_bytes, num_padded_bits, original_length)\n",
    "compressed_bytes, num_padded_bits, original_length = read_padded_bytes(\"compressed.bin\")\n",
    "print(compressed_bytes)\n",
    "print(num_padded_bits)\n",
    "print(original_length)\n",
    "\n",
    "decompression_start_time = time.time()\n",
    "\n",
    "decompressed = decode(\n",
    "    compressed_bytes,\n",
    "    num_padded_bits,\n",
    "    llm,\n",
    "    start_symbol,\n",
    "    device,\n",
    "    original_length,\n",
    "    sequence_array,\n",
    "    pd,\n",
    "    probs,\n",
    "    do_test=True,\n",
    ")\n",
    "\n",
    "decompression_end_time = time.time()\n",
    "\n",
    "print(tokenized[\"input_ids\"].squeeze(0).numpy())\n",
    "print(decompressed)\n",
    "\n",
    "print(f\"Compression time: {compression_end_time - compression_start_time:.2f} seconds\")\n",
    "print(f\"Decompression time: {decompression_end_time - decompression_start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LMCompress+",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
